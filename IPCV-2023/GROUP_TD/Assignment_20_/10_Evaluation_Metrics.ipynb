{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hiện thực confusion matrix và classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_class = 15\n",
    "num_sample = 5000\n",
    "y_true = np.random.randint(num_class, size = num_sample)  \n",
    "y_pred = np.random.randint(num_class, size = num_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_confusion_matrix(y_true, y_pred, num_classes):   \n",
    "    num_classes = len(np.unique(y_true))\n",
    "    conf_matrix = np.zeros((num_classes, num_classes))\n",
    "    for i in range(len(y_true)):\n",
    "        conf_matrix[y_true[i]][y_pred[i]] += 1\n",
    "    return conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def recall(conf_matrix):\n",
    "    return np.diag(conf_matrix) / np.sum(conf_matrix, axis = 1)\n",
    "\n",
    "def precision(conf_matrix):\n",
    "    return np.diag(conf_matrix) / np.sum(conf_matrix, axis = 0)\n",
    "\n",
    "def f1_score(conf_matrix):\n",
    "    p = precision(conf_matrix)\n",
    "    r = recall(conf_matrix)\n",
    "    return 2 * p * r / (p + r)\n",
    "\n",
    "def accuracy(conf_matrix):\n",
    "    return np.trace(conf_matrix) / np.sum(conf_matrix)\n",
    "\n",
    "def metric_panda_report(conf_matrix):\n",
    "    report = pd.DataFrame()\n",
    "    report['recall'] = recall(conf_matrix)\n",
    "    report['precision'] = precision(conf_matrix)\n",
    "    report['f1_score'] = f1_score(conf_matrix)\n",
    "    report['support'] = np.sum(conf_matrix, axis = 1)\n",
    "    return report\n",
    "\n",
    "def get_weighted_avg(report, name_metric):\n",
    "    return np.sum(report[name_metric] * report['support']) / np.sum(report['support'])\n",
    "\n",
    "def get_macro_avg(report, name_metric):\n",
    "    return np.mean(report[name_metric])\n",
    "\n",
    "def final_report(report, conf_matrix, round_digit = 2):\n",
    "    num_sample = np.sum(conf_matrix)\n",
    "    report.loc['accuracy'] = accuracy(conf_matrix)\n",
    "    report.loc['weighted avg'] = report.apply(lambda x: get_weighted_avg(report, x.name), axis = 0)\n",
    "    report.loc['macro avg'] = report.apply(lambda x: get_macro_avg(report, x.name), axis = 0)\n",
    "    report['support'] = report['support'].astype(int)\n",
    "    report = report.round(round_digit)\n",
    "    report.loc['accuracy', 'support'] = num_sample\n",
    "    report.loc['weighted avg', 'support'] = num_sample\n",
    "    report.loc['macro avg', 'support'] = num_sample\n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              recall  precision  f1_score  support\n",
      "0               0.08       0.08      0.08      330\n",
      "1               0.08       0.08      0.08      324\n",
      "2               0.06       0.05      0.06      318\n",
      "3               0.07       0.06      0.07      333\n",
      "4               0.09       0.10      0.09      329\n",
      "5               0.06       0.06      0.06      314\n",
      "6               0.05       0.05      0.05      320\n",
      "7               0.05       0.05      0.05      329\n",
      "8               0.05       0.05      0.05      355\n",
      "9               0.07       0.07      0.07      326\n",
      "10              0.07       0.07      0.07      349\n",
      "11              0.07       0.06      0.07      343\n",
      "12              0.09       0.09      0.09      335\n",
      "13              0.07       0.07      0.07      342\n",
      "14              0.08       0.08      0.08      353\n",
      "accuracy        0.07       0.07      0.07     5000\n",
      "weighted avg    0.07       0.07      0.07     5000\n",
      "macro avg       0.07       0.07      0.07     5000\n"
     ]
    }
   ],
   "source": [
    "basic_report = metric_panda_report(multiclass_confusion_matrix(y_true, y_pred, num_class))\n",
    "my_classification_report = final_report(basic_report, multiclass_confusion_matrix(y_true, y_pred, num_class))\n",
    "print(my_classification_report)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Đối chiếu với sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.08      0.08      0.08       330\n",
      "           1       0.08      0.08      0.08       324\n",
      "           2       0.05      0.06      0.06       318\n",
      "           3       0.06      0.07      0.07       333\n",
      "           4       0.10      0.09      0.09       329\n",
      "           5       0.06      0.06      0.06       314\n",
      "           6       0.05      0.05      0.05       320\n",
      "           7       0.05      0.05      0.05       329\n",
      "           8       0.05      0.05      0.05       355\n",
      "           9       0.07      0.07      0.07       326\n",
      "          10       0.07      0.07      0.07       349\n",
      "          11       0.06      0.07      0.07       343\n",
      "          12       0.09      0.09      0.09       335\n",
      "          13       0.07      0.07      0.07       342\n",
      "          14       0.08      0.08      0.08       353\n",
      "\n",
      "    accuracy                           0.07      5000\n",
      "   macro avg       0.07      0.07      0.07      5000\n",
      "weighted avg       0.07      0.07      0.07      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sklearn_report = sklearn.metrics.classification_report(y_true, y_pred)\n",
    "print(sklearn_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
